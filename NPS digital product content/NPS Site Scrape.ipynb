{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volpe Center\n",
    "### Code for National Park Service sponsor project\n",
    "### Code by Eric Englin\n",
    "### Date: 10/23/23\n",
    "<br><br>\n",
    "#### Objective: scrape all websites for NPS to see if they meet the 10 essential travelers information for transportation\n",
    "#### These 10 are:\n",
    "<li>driving directions</li>\n",
    "<li>Public transportation information</li>\n",
    "<li>Bike and pedestrian information</li>\n",
    "<li>Parking lot locations and accommodations</li>\n",
    "<li>Parking lot peak use and availability</li>\n",
    "<li>congestion information</li>\n",
    "<li>travel distances and travel time to sites within the park</li>\n",
    "<li>Accessibility</li>\n",
    "<li>Description of transportation experience</li>\n",
    "<li>Alternative fueling stations</li>\n",
    "\n",
    "<br><br>\n",
    "#### Together, these 10 measures can allow NPS to understand how their parks are providing transportation to visitors. This information can be used to evaluate each park and plan for an improved park experience in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns \n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from VE_scraper_functions import *\n",
    "from chromedriver_py import binary_path # this will get you the path variable\n",
    "from selenium.webdriver.common.by import By\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/76727774/selenium-webdriver-chrome-115-stopped-working\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change to the location where you saved chromedriver\n",
    "chromedriver_location=r'C:\\Users\\eric.englin\\Downloads/chromedriver.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get('https://www.nps.gov/AGFO/planyourvisit/directions.htm')\n",
    "driver.close() #close driver link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Park Unit Scraping Information.csv\"\n",
    "parks = pd.read_csv(path, encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "for x in parks['Alpha']:\n",
    "    y = \"https://www.nps.gov/\"+x+\"/index.htm\"\n",
    "    index.append(y)\n",
    "\n",
    "parks['index site']=index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For context, here is the main site for each national park\n",
    "for x in parks['index site']:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(link)\n",
    "page_location = driver.find_element(By.ID, \"breadcrumbs\")\n",
    "print(page_location.text)\n",
    "elems = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "for elem in elems:\n",
    "    try:\n",
    "        if \"planyourvisit\" in str(elem.get_attribute('href')): #only want plan your visit sites\n",
    "            print(elem.get_attribute('href'))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for elem in elems:\n",
    "    print(elem.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service()\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "park = \"YOSE\"\n",
    "link = \"https://www.nps.gov/\"+park+\"/index.htm\"\n",
    "driver.get(link)\n",
    "#driver.find_element_by_xpath('//*[@id=\"anch_15\"]').click()\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "elems = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "website_list=[]\n",
    "raw_list=[]\n",
    "website_content=[]\n",
    "raw_soup_list = []\n",
    "website_location = []\n",
    "for elem in elems:\n",
    "    try:\n",
    "        if \"planyourvisit\" in str(elem.get_attribute('href')) and str(elem.get_attribute('href')) not in raw_list: #only want plan your visit sites\n",
    "            z= str(elem.get_attribute('href'))\n",
    "            raw_list.append(z)\n",
    "            website_list.append(z)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for x in website_list:\n",
    "    try:\n",
    "        driver.get(x)\n",
    "        \n",
    "        page_location = driver.find_element(By.ID, \"breadcrumbs\")\n",
    "        website_location.append(page_location.text)\n",
    "        page_source = driver.page_source\n",
    "        elems = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "        raw_content = soup.get_text(strip=True) #all text fields are scraped\n",
    "        website_content.append(raw_content) #raw content added to list of all content\n",
    "        raw_soup_list.append(soup)\n",
    "    except:\n",
    "        #This means that the webpage doesn't exist\n",
    "        website_content.append(\"page doesn't exist\")\n",
    "        raw_soup_list.append(\"page doesn't exist\")\n",
    "        website_location.append(\"page doesn't exist\")\n",
    "\n",
    "driver.close()\n",
    "\n",
    "dict = {'website page': website_list, 'content': website_content, 'soup': raw_soup_list,\n",
    "       \"website location\": website_location}  #create dataframe for park data\n",
    "park_data = pd.DataFrame(dict)\n",
    "park_data['park']=park\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \n",
    "             \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \n",
    "             \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \n",
    "             \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\",\n",
    "             \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \n",
    "             \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \n",
    "             \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \n",
    "             \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "             \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \n",
    "             \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \n",
    "             \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \n",
    "             \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \n",
    "             \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \n",
    "             \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\",\n",
    "            \"href\",\"=\",\"/\",\">\",\"<\",\"]\",\"[\",\"span\",\"'\\n'\",'class',\"jstcache\",\n",
    "            \"onclick\",\"null\",\"jscontent\",\" <br/>\",\"</span>\",\",\",\";\",\"(\",\")\",\"{\",\"}\",\":\",\"''\",\n",
    "            \"&\",\"'\",\"var\",\"+=\",\".\",\"#\",\"-\",\"=\",\"+\",\"``\",\"0\",\"â€™\",\"data.operatingHours\",\"outputVarOperatingHours\",\n",
    "            \".exceptions\",\"--\",\"1\",\"-1\",\"?\",\"class=\",\"==\",\"div\",\"/div\",\"$\",\"li\",\"e\",\"!\",\"k\",\"/span\",\"jQuery\",\n",
    "            \"tabindex\",'j','l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=0\n",
    "v=0\n",
    "for x in parks['Alpha'].unique():\n",
    "    v+=1\n",
    "    if z==0:\n",
    "        park_scrape_dataset=scrape_site(x)\n",
    "        print(x)\n",
    "       # break\n",
    "    else:\n",
    "        this_park_scrape = scrape_site(x)\n",
    "        park_scrape_dataset = park_scrape_dataset.append(this_park_scrape)\n",
    "    if v % 25 == 0:\n",
    "        print(x)\n",
    "        print(len(park_scrape_dataset))\n",
    "    z+=1\n",
    "    #if z>50: #if want to test out\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your scraped website into another variable name so don't have to redo scrape\n",
    "park_scrape_dataset2 = park_scrape_dataset\n",
    "\n",
    "#data cleaning\n",
    "park_scrape_dataset2['index1'] = park_scrape_dataset2.index\n",
    "park_scrape_dataset2=park_scrape_dataset2.reset_index()\n",
    "\n",
    "\n",
    "#save as excel\n",
    "#note: saving as a csv won't work due to punctuation used in html code\n",
    "park_scrape_dataset2.to_excel(\"full_park_scrape_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "park_scrape_dataset2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_scrape_dataset2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_scrape_dataset_content = park_scrape_dataset2[['index','website page', 'content', 'website location', 'park']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "park_scrape_dataset_content.to_excel(\"park_scrape_content_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model to calculate VE fields ##\n",
    "\n",
    "#create new sheet so with our variables for each park\n",
    "park_sheet = pd.DataFrame(columns = ['park', 'Driving_Directions','Public_transportation_information',\n",
    "                                     'Bike_Pedestrian_Information','Congestion_information','Accessibility',\n",
    "                                           'Alternative_Fueling_Stations', 'website page count'])\n",
    "z=0\n",
    "tic = time.process_time() #function to let us track processing time\n",
    "\n",
    "\n",
    "for x in park_scrape_dataset2['park'].unique():\n",
    "    z+=1\n",
    "    if z % 25 == 0: \n",
    "        #function to let us track processing time\n",
    "        z5 = 424-z\n",
    "        toc = time.process_time()\n",
    "        time_diff = toc-tic\n",
    "        print(\"Current Park: \", x, \": \", z, \" checks done; \", z5, \" remaining; Processing Time: \",time_diff)\n",
    "        tic=toc\n",
    "        \n",
    "    this_park = park_scrape_dataset2[(park_scrape_dataset2['park']==x)] #filter our webscraping dataset for our park's website code\n",
    "    park_final = Traveler_Info_Finder(this_park) #run function\n",
    "    park_sheet = park_sheet.append({'park': park_final.get_value(0,'park'),\n",
    "                        'website page count': park_final.get_value(0,'website page'),\n",
    "                        'Directions_word_count': park_final.get_value(0,'Directions_count'),\n",
    "                        'Directions_page_count': park_final.get_value(0,'Directions_page_count'),\n",
    "                       'Driving_Directions': park_final.get_value(0,'MajorDirections_count'),\n",
    "                       'Public_transportation_information': park_final.get_value(0,'Public_transportation_information'),\n",
    "                       'Alternative_Fueling_Stations': park_final.get_value(0,'Alternative_Fueling_Stations'), \n",
    "                       'Bike_Pedestrian_Information': park_final.get_value(0,'Bike_Pedestrian_Information'),\n",
    "                       'Congestion_information': park_final.get_value(0,'Congestion_information'),\n",
    "                        'Travel_Distance_Information': park_final.get_value(0,'Travel_dist_information'),\n",
    "                        'Travel_other_dist_information': park_final.get_value(0,'Travel_other_dist_information'),\n",
    "                        'Accessibility': park_final.get_value(0,'Accessibility_information'),\n",
    "                        'Parking_raw_information': park_final.get_value(0,'Parking_information'),\n",
    "                        'Parking_experience_information': park_final.get_value(0,'Parking_experience_information'),\n",
    "                        'Parking_max_on_one_site': park_final.get_value(0,'Parking_max_on_one_site')\n",
    "                       },\n",
    "                      ignore_index=True)\n",
    "    park_sheet.loc[park_sheet.Driving_Directions > 0, 'Driving_Directions'] = 1\n",
    "    park_sheet.loc[park_sheet.Alternative_Fueling_Stations > 0, 'Alternative_Fueling_Stations'] = 1\n",
    "    park_sheet.loc[park_sheet.Public_transportation_information > 0, 'Public_transportation_information'] = 1\n",
    "    park_sheet.loc[park_sheet.Bike_Pedestrian_Information > 0, 'Bike_Pedestrian_Information'] = 1\n",
    "    park_sheet.loc[park_sheet.Congestion_information > 0, 'Congestion_information'] = 1\n",
    "    park_sheet.loc[park_sheet.Accessibility > 0, 'Accessibility'] = 1\n",
    " #   park_sheet.loc[park_sheet.Parking_information > 0, 'Parking_information'] = 1\n",
    "    park_sheet['Travel_Distance_Final']=np.where(\n",
    "        np.logical_or(park_sheet['Travel_Distance_Information']>9, \n",
    "                     park_sheet['Travel_other_dist_information']>0),1,0)\n",
    "    park_sheet['Parking_Experience_information']=np.where((\n",
    "        park_sheet['Parking_raw_information']/park_sheet['website page count'])>0.25,1,0)\n",
    "    park_sheet['Transportation_experience_information']=np.where((\n",
    "        park_sheet['Directions_page_count']/park_sheet['website page count'])>0.65,1,0)\n",
    "    park_sheet['Parking_information']=np.where(np.logical_or(\n",
    "        park_sheet['Parking_Experience_information']==1,\n",
    "        park_sheet['Parking_max_on_one_site']>2),1,0)\n",
    "\n",
    "\n",
    "park_sheet= park_sheet.drop(columns=['website page count', 'Directions_word_count',\n",
    "                        'Directions_page_count','Parking_raw_information','Parking_experience_information',\n",
    "                        'Parking_max_on_one_site','Travel_Distance_Information','Travel_other_dist_information'])\n",
    "    \n",
    "#create csv\n",
    "park_sheet.to_csv(\"final_park.csv\") #save final csv\n",
    "os.system(\"start EXCEL.EXE final_park.csv\") #open csv file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
